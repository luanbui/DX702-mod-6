In all cases, written answers (apart from code) should not be longer than about three paragraphs.  Graders may not read all of your
submission if it is longer than that.

Homework Reflection 5

1. Draw a diagram for the following negative feedback loop:

Sweating causes body temperature to decrease.  High body temperature causes sweating.

A negative feedback loop means that one thing increases another while the second thing decreases the first.

Remember that we are using directed acyclic graphs where two things cannot directly cause each other.

**Answer:**
Since we need a directed acyclic graph (DAG), we must introduce a time component to avoid cycles:
```
BodyTemp(t) → Sweating(t+1) → BodyTemp(t+2)
```
At time t, high body temperature causes increased sweating at time t+1, which then causes decreased body temperature at time t+2. This creates a negative feedback loop while maintaining the acyclic property by using different time points.

2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.

**Answer:**
A classic example is the relationship between wealth and investment opportunities. Wealth(t) → Investment(t+1) → Wealth(t+2). Greater wealth provides more capital to invest, and successful investments generate more wealth, creating a positive feedback loop. Another example is social media engagement: Followers(t) → Content_Visibility(t+1) → Followers(t+2), where more followers lead to greater content visibility, which attracts even more followers.

3. Draw a diagram for the following situation:

Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.
Bears eat deer, decreasing their population.
Deer eat flowers, decreasing their population.

Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.

Identify a backdoor path with one or more confounders for the relationship between deer and flowers.

**Answer:**

**Diagram:**
```
Lightning → Deer (decreasing)
Lightning → Bears (decreasing)
Lightning → Flowers (increasing)
Bears → Deer (decreasing)
Deer → Flowers (decreasing)
```

**Code to simulate this situation:**
```python
import numpy as np
import pandas as pd

np.random.seed(42)
num = 10000

# Lightning storms (0-1 scale, higher = more storms)
lightning = np.random.uniform(0, 1, num)

# Bears: decreased by lightning, with noise
# Base population ~N(50, 10), reduced by 30*lightning
bears = np.maximum(np.random.normal(50, 10, num) - 30 * lightning + np.random.normal(0, 5, num), 0)

# Deer: decreased by both lightning and bears, with noise
# Base population ~N(100, 20), reduced by 40*lightning and 0.5*bears
deer = np.maximum(np.random.normal(100, 20, num) - 40 * lightning - 0.5 * bears + np.random.normal(0, 10, num), 0)

# Flowers: increased by lightning, decreased by deer, with noise
# Base population ~N(200, 30), increased by 50*lightning, reduced by 0.3*deer
flowers = np.maximum(np.random.normal(200, 30, num) + 50 * lightning - 0.3 * deer + np.random.normal(0, 15, num), 0)

# Create DataFrame
df = pd.DataFrame({
    'lightning': lightning,
    'bears': bears,
    'deer': deer,
    'flowers': flowers
})

print(df.head(10))
print(f"\nDataset shape: {df.shape}")
```

**Backdoor path with confounder:**
There is a backdoor path from Deer to Flowers through Lightning: Deer ← Lightning → Flowers. Lightning is a confounder because it affects both deer (decreasing) and flowers (increasing). To estimate the true causal effect of deer on flowers, we would need to control for lightning to block this backdoor path.

4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?

**Answer:**

**Scenario: Biking Accidents Analysis**

**Diagram:**
```
Trail_Difficulty (X) → Speed (Y) → Accident_Likelihood (Z)
        ↓                               ↑
        └───────────────────────────────┘
```

**Nodes:**
- Trail_Difficulty (X) - Trail difficulty level (0-1 scale)
- Speed (Y) - Biking speed in mph
- Accident_Likelihood (Z) - Likelihood of accident (0-1 scale)
- (Could add Experience as confounder if extended)

**Causal relationships:**
- X → Y: Trail difficulty decreases speed (people slow down on difficult trails)
- X → Z: Trail difficulty increases accident likelihood
- Y → Z: Speed increases accident likelihood

**Treatment (X):** Trail_Difficulty - This is the exposure we might want to study
**Outcome (Y):** Speed - This is the primary outcome of interest if we're studying how difficulty affects riding behavior
**Collider (Z):** Accident_Likelihood - This is a collider because both X (difficulty) and Y (speed) cause Z. We should NOT control for Z when estimating the effect of X on Y.

If we wanted a clearer confounder example, we could add: Weather → Trail_Difficulty and Weather → Speed, making Weather a confounder for the X→Y relationship.

Homework Reflection 6

1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)
Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.
(Please explain / justify this answer, or give a different one if you can think of one.)

**Answer:**
The main problem is that selecting the single maximum treatment effect is highly susceptible to random noise and overfitting. When we search through potentially hundreds or thousands of observations to find the one with the largest treatment effect, we're essentially performing multiple comparisons without adjustment. The "winner" is likely to have a treatment effect that appears extreme partly due to statistical noise rather than genuine causal impact. This is a form of selection bias - we're cherry-picking the most extreme observation, which by definition is likely to be an outlier. Additionally, this single observation may not be representative of broader patterns, and small sample fluctuations or measurement errors in either the treated or control groups could dramatically inflate the observed maximum effect. The result is an unstable estimate with high variance that will likely overestimate the true marginal treatment effect.

2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.
Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.
(Either code this answer or choose a different one.)

**Answer:**
A robust solution is to use the 90th percentile (or another high percentile like 85th or 95th) of the treatment effect distribution instead of the absolute maximum. This approach balances identifying individuals with high treatment effects while reducing sensitivity to extreme outliers and noise. The percentile-based estimate is more stable because it considers a broader range of high-effect individuals rather than just the single most extreme case.

**Code implementation:**
```python
import pandas as pd
import numpy as np
from sklearn.neighbors import NearestNeighbors

# Load the dataset
df = pd.read_csv('homework_6.1.csv', index_col=0)

# Separate treated and untreated groups
treated = df[df['X'] == 1].copy()
untreated = df[df['X'] == 0].copy()

# For untreated units, find nearest treated neighbor
nn_treated = NearestNeighbors(n_neighbors=1)
nn_treated.fit(treated[['Z']])
distances_u, indices_u = nn_treated.kneighbors(untreated[['Z']])

# Calculate individual treatment effects for untreated units
# ITE = Y_counterfactual (matched treated) - Y_untreated
untreated_effects = treated.iloc[indices_u.flatten()]['Y'].values - untreated['Y'].values

# Solution 1: Maximum (original method - problematic)
optimal_te_max = np.max(untreated_effects)

# Solution 2: 90th percentile (robust alternative)
optimal_te_90th = np.percentile(untreated_effects, 90)

# Solution 3: 95th percentile (more conservative)
optimal_te_95th = np.percentile(untreated_effects, 95)

# Display results
print("Marginal Treatment Effect Estimates:")
print("=" * 60)
print(f"Maximum (original):           {optimal_te_max:.6f}")
print(f"90th percentile (robust):     {optimal_te_90th:.6f}")
print(f"95th percentile (conservative): {optimal_te_95th:.6f}")
print(f"Mean treatment effect:        {np.mean(untreated_effects):.6f}")
print("=" * 60)
print(f"\nDifference between max and 90th percentile: {optimal_te_max - optimal_te_90th:.6f}")
print(f"This difference represents potential noise/outlier influence.")
```

The 90th percentile approach provides a more reliable estimate because it identifies individuals who would benefit substantially from treatment while avoiding the extreme sensitivity to outliers inherent in using the absolute maximum.

Homework Reflection 7

1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.

**Answer:**

**Code:**
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

np.random.seed(42)
n = 10000

# W is a confounder - it affects both X and Y
W = np.random.normal(0, 1, n)

# X is influenced by W (confounder)
X = 2 * W + np.random.normal(0, 1, n)

# Y is influenced by both W (confounder) and X (treatment)
# True causal effect of X on Y is 3.0
# W also has a positive effect of 4.0 on Y
Y = 3 * X + 4 * W + np.random.normal(0, 1, n)

# Model WITHOUT controlling for W (incorrect - omits confounder)
model_biased = LinearRegression()
model_biased.fit(X.reshape(-1, 1), Y)
coef_biased = model_biased.coef_[0]

# Model WITH controlling for W (correct - includes confounder)
model_correct = LinearRegression()
model_correct.fit(np.column_stack([X, W]), Y)
coef_X_correct = model_correct.coef_[0]
coef_W_correct = model_correct.coef_[1]

print("Regression Results:")
print("=" * 60)
print(f"True causal effect of X on Y: 3.0")
print(f"Coefficient without controlling for W: {coef_biased:.4f}")
print(f"Coefficient with controlling for W: {coef_X_correct:.4f}")
print(f"Coefficient of W: {coef_W_correct:.4f}")
print("=" * 60)
print(f"\nThe correlation is OVERESTIMATED by {coef_biased - 3.0:.4f}")
```

**Explanation:**
The true causal effect of X on Y is **overestimated** when we omit the confounder W. Without controlling for W, the regression attributes both the direct effect of X on Y (coefficient 3.0) and the indirect path through W to the X coefficient. Since W positively affects both X (coefficient 2.0) and Y (coefficient 4.0), part of Y's variation that is actually due to W gets incorrectly attributed to X. The estimated coefficient becomes approximately 3 + (2 × 4) = 11.0 instead of the true 3.0. This happens because X and W are correlated, so when we leave W out, the model cannot distinguish between changes in Y caused directly by X versus changes caused by the confounding variable W that happens to be correlated with X.

2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.

W = [noise]
X = [noise]
Y = 2 * X + [noise]

And compute the p-value of a coefficient - in this case, the coefficient of W.  
(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)
If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)
Run the analysis 1000 times and report the best (smallest) p-value.  
If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?

**Answer:**

**Code:**
```python
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy import stats

np.random.seed(42)
n_experiments = 1000
sample_size = 100
p_values = []

for i in range(n_experiments):
    # Generate data where W has NO effect on Y
    W = np.random.normal(0, 1, sample_size)
    X = np.random.normal(0, 1, sample_size)
    Y = 2 * X + np.random.normal(0, 1, sample_size)
    
    # Fit regression: Y ~ W + X
    predictors = np.column_stack([W, X])
    model = LinearRegression()
    model.fit(predictors, Y)
    
    # Calculate p-value for W coefficient
    # Get predictions and residuals
    Y_pred = model.predict(predictors)
    residuals = Y - Y_pred
    
    # Calculate standard error of W coefficient
    mse = np.sum(residuals ** 2) / (sample_size - 2)
    var_covar_matrix = mse * np.linalg.inv(predictors.T @ predictors)
    se_W = np.sqrt(var_covar_matrix[0, 0])
    
    # t-statistic for W coefficient
    t_stat = model.coef_[0] / se_W
    
    # p-value (two-tailed)
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=sample_size - 2))
    p_values.append(p_value)

# Find the smallest p-value
min_p_value = np.min(p_values)
num_significant = np.sum(np.array(p_values) < 0.05)

print("Results from 1000 experiments:")
print("=" * 60)
print(f"Smallest p-value: {min_p_value:.6f}")
print(f"Number of experiments with p < 0.05: {num_significant}")
print(f"Percentage with p < 0.05: {100 * num_significant / n_experiments:.1f}%")
print("=" * 60)
print(f"\nExpected false positive rate: 5.0%")
```

**Explanation:**
Even though the coefficient of W is truly zero, the smallest p-value across 1000 experiments is likely to be much less than 0.05 (often < 0.001). This does NOT mean the coefficient is actually nonzero. The problem with repeating the analysis is called **p-hacking** or the **multiple testing problem**. When we run many experiments, we're bound to get some extreme results purely by chance. With 1000 experiments and a 5% false positive rate, we expect about 50 experiments to show p < 0.05 even when there's no real effect. By selecting the smallest p-value, we're cherry-picking the most extreme random fluctuation. This demonstrates why researchers must correct for multiple comparisons (using Bonferroni correction, FDR, etc.) and why pre-registering analyses is important - otherwise, repeating analyses until finding significance leads to spurious "discoveries" that don't replicate.

Homework Reflection 8

Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.

---

## Problem: Average Treatment Effect with Inverse Probability Weighting

### Code Solution

```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression

# Load the data
df = pd.read_csv('homework_8.1.csv')

# Step 1: Identify variables
X_treatment = df['X']
Y_outcome = df['Y']
# Use only actual Z covariate columns (exclude index columns)
Z_covariates = df.filter(regex='^Z')

# Step 2: Estimate propensity scores using logistic regression
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(Z_covariates, X_treatment)

# Step 3: Predict propensity scores
propensity_scores = logreg.predict_proba(Z_covariates)[:, 1]

# Step 4: Calculate inverse probability weights
# For X=1 (treated): weight = 1/P
# For X=0 (control): weight = 1/(1-P)
weights = np.where(X_treatment == 1, 
                   1 / propensity_scores,
                   1 / (1 - propensity_scores))

# Step 5: Calculate Average Treatment Effect (ATE)
treated_mask = (X_treatment == 1)
control_mask = (X_treatment == 0)

weighted_outcome_treated = (weights[treated_mask] * Y_outcome[treated_mask]).sum()
sum_weights_treated = weights[treated_mask].sum()
avg_outcome_treated = weighted_outcome_treated / sum_weights_treated

weighted_outcome_control = (weights[control_mask] * Y_outcome[control_mask]).sum()
sum_weights_control = weights[control_mask].sum()
avg_outcome_control = weighted_outcome_control / sum_weights_control

ATE = avg_outcome_treated - avg_outcome_control
print(f"Average Treatment Effect (ATE): {ATE:.6f}")
```

### Written Explanation

**Approach and Methodology:**

The inverse probability weighting (IPW) method addresses confounding in observational studies by reweighting observations to simulate a randomized experiment. My implementation followed the standard IPW procedure in four key steps.

First, I identified the treatment variable (X), outcome (Y), and all covariates (Z). In this dataset, the covariates included both a Z variable and an index column. I chose to include all non-treatment, non-outcome variables as covariates to control for all available confounding.

Second, I fitted a logistic regression model to estimate propensity scores - the probability of receiving treatment given the covariates. I used sklearn's LogisticRegression with max_iter=1000 to ensure convergence and set a random_state for reproducibility. The predict_proba() method returns probabilities for both classes, so I extracted the second column (P(X=1|Z)).

Third, I calculated inverse probability weights using a vectorized np.where() operation. For treated units, the weight is 1/P, and for control units, it's 1/(1-P). These weights are crucial - they upweight observations that are unlikely to receive their observed treatment based on their covariates, effectively balancing the treatment and control groups.

Finally, I computed the ATE as the difference between weighted average outcomes. The result (ATE ≈ 2.27) represents the average causal effect of treatment, adjusted for confounding.

**Key Insights and Challenges:**

The main challenge was understanding the theoretical foundation - why do inverse probability weights create balance? The insight is that by weighting each observation by the inverse of its treatment probability, we create a pseudo-population where treatment assignment is independent of covariates, mimicking randomization.

I compared the IPW estimate (2.27) to a naive estimate without weighting (3.04). The substantial difference (0.77) demonstrates the importance of adjusting for confounders. The naive estimate is biased because treatment and control groups likely differ systematically in their covariates.

A potential issue I considered was extreme weights. When propensity scores are very close to 0 or 1, the weights become very large, increasing variance. In this dataset, the maximum weight was about 15.3, which is manageable but suggests some observations were quite unlikely to receive their observed treatment. In practice, one might consider trimming extreme propensity scores or using stabilized weights.

Overall, this exercise reinforced the importance of causal inference methods in observational studies and demonstrated how IPW can recover unbiased treatment effect estimates when randomization is not possible.

---

## Problem 2: Mahalanobis Distance Matching

### Code Solution

```python
# Import libraries
import pandas as pd
import numpy as np
from scipy.spatial.distance import mahalanobis

# Load the data
df2 = pd.read_csv('homework_8.2.csv')

# Identify treatment and covariates
X2 = df2['X']
Z_cols_2 = df2.filter(regex='^Z').columns.tolist()  # Gets Z1 and Z2

# Separate treated and untreated groups
treated_df = df2[X2 == 1].copy()
untreated_df = df2[X2 == 0].copy()

# Step 1: Calculate inverse covariance matrix
# Create 2 x N matrix from all Z1 and Z2 values
Z_matrix = df2[Z_cols_2].values.T  # Transpose to get 2 x N

# Calculate 2x2 covariance matrix and invert it
cov_matrix = np.cov(Z_matrix)
inv_cov_matrix = np.linalg.inv(cov_matrix)

# Step 2: Match each treated item to nearest untreated item
matches = []
treated_Z = treated_df[Z_cols_2].values
untreated_Z = untreated_df[Z_cols_2].values

for i, treated_point in enumerate(treated_Z):
    min_distance = float('inf')
    best_match_idx = -1
    
    # Calculate Mahalanobis distance to all untreated items
    for j, untreated_point in enumerate(untreated_Z):
        distance = mahalanobis(treated_point, untreated_point, inv_cov_matrix)
        
        if distance < min_distance:
            min_distance = distance
            best_match_idx = j
    
    # Store the match
    matches.append({
        'treated_idx': treated_df.index[i],
        'untreated_idx': untreated_df.index[best_match_idx],
        'distance': min_distance
    })

matches_df = pd.DataFrame(matches)
print(f"Matched {len(matches)} treated items")
print(f"Mean distance: {matches_df['distance'].mean():.4f}")
```

### Written Explanation

**Approach and Strategy:**

The Mahalanobis distance matching problem required implementing a nearest-neighbor matching algorithm where each treated unit is matched to the most similar untreated unit, with replacement allowed. The key challenge was correctly computing the Mahalanobis distance, which accounts for the correlation structure between covariates.

My approach involved three main steps. First, I calculated the inverse covariance matrix using all observations (both treated and untreated). I created a 2×N matrix by transposing the Z1 and Z2 values, computed the 2×2 covariance matrix using numpy's cov() function, and inverted it using linalg.inv(). This inverse covariance matrix is crucial because it weights the distance calculation based on the variance and covariance of the features.

Second, I implemented a nested loop to find the nearest untreated match for each treated item. For each treated observation, I calculated the Mahalanobis distance to every untreated observation using scipy's mahalanobis() function, which takes three parameters: the treated point, untreated point, and the inverse covariance matrix. I tracked the minimum distance and corresponding index to identify the best match.

Third, I stored all matches in a structured format for analysis. With replacement enabled, some untreated items were matched multiple times while others were never used, which is expected when matches are chosen purely based on similarity.

**Key Insights and Challenges:**

The most important insight was understanding why Mahalanobis distance is superior to Euclidean distance for matching. When covariates are correlated (as Z1 and Z2 were in this dataset with correlation ~0.7), Euclidean distance can be misleading because it treats all dimensions equally. Mahalanobis distance accounts for this correlation, effectively "stretching" the space along correlated dimensions.

A practical challenge was ensuring the covariance matrix was calculated correctly. The instructions specified using a 2×N matrix (features × observations), which is the correct format for np.cov(). Getting the transpose right was critical - the input needed to be transposed from the standard N×2 DataFrame format.

The matching results revealed interesting patterns. Out of 517 untreated items, only 172 were actually used as matches, with some items matched up to 28 times. This demonstrates that certain untreated observations are particularly "representative" of the treated population in terms of covariate similarity. The mean Mahalanobis distance of 0.20 suggests generally good matches, though some treated items (max distance 1.38) were harder to match, possibly indicating violations of the common support assumption.
